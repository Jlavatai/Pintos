            +-------------------+
            |       OS 211      |
            |  TASK 1: THREADS  |
            |  DESIGN DOCUMENT  |
            +-------------------+
                   
---- GROUP ----

>> Fill in the names and email addresses of your group members.

Francesco Di Mauro <fd1011@imperial.ac.uk>
Thomas Rooney <tr111@imperial.ac.uk>
Alex Rozanski <anr11@imperial.ac.uk>

---- PRELIMINARIES ----

>> If you have any preliminary comments on your submission, notes for the
>> TAs, or extra credit, please give them here.

>> Please cite any offline or online sources you consulted while
>> preparing your submission, other than the Pintos documentation, course
>> text, lecture notes, and course staff.

                 ALARM CLOCK
                 ===========

---- DATA STRUCTURES ----

>> A1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

Added to enum thread_status:

THREAD_SLEEP            /* Indicates the thread is sleeping. */

Added to struct thread:

long long wakeup_tick;  /* If sleeping, the tick we want to wake up on. */

Added to thread.c:

/* List of processes currently sleeping. Processes in this list 
   have state set to THREAD_SLEEP. Each timer interrupt, this
   is decremented until it reaches 0, upon which they are taken 
   out of the sleeping list. */
static struct list sleeping_list;

---- ALGORITHMS ----

>> A2: Briefly describe what happens in a call to timer_sleep(),
>> including the effects of the timer interrupt handler.

timer_sleep() calls out to a new method we implemented, thread_sleep().
This adds the current thread to a list of sleeping threads which we maintain.

>> A3: What steps are taken to minimize the amount of time spent in
>> the timer interrupt handler?

The list of sleeping threads we maintain is ordered by the wakeup_tick member
which we added to struct thread. This is an absolute tick value that the thread 
should sleep until, which is set when timer_sleep() is invoked. When we iterate
over the sleeping threads in thread_sleep_ticker() (which is called from the
timer interrupt handler) we can stop iteration once we hit a thread whose
wakeup_tick value is later than the current tick we're on, because of this
ordering.

Furthermore, because the tick which we want the thread to wake up on is stored
as an absolute value rather than a relative number of ticks to sleep for, we 
don't need to update any sleep state in the timer interrupt handler for the 
sleeping threads.

---- SYNCHRONIZATION ----

>> A4: How are race conditions avoided when multiple threads call
>> timer_sleep() simultaneously?

We disable interrupts in timer_sleep() before calling thread_sleep() (which is 
the critical section where we modify the sleeping list and then schedule 
another thread). Since interrupts are disabled for this small section, we won't 
be pre-empted by another thread, so sleeping a thread and then scheduling 
another is an atomic operation.

We switch to another thread when the current thread sleeps by calling
schedule(), so although we could use a synchronisation primitive like a
semaphore to enforce invocation of thread_sleep() as a critical section,
schedule() asserts that interrupts are disabled, so it makes the most sense to
disable interrupts for this critical section as they need to be off for
schedule() anyway.

>> A5: How are race conditions avoided when a timer interrupt occurs
>> during a call to timer_sleep()?

As we disable interrupts before calling thread_sleep() from timer_sleep(), this
ensures that timer interrupts aren't handled during the invocation of this
function which prevents race conditions occuring between multiple threads where
we may wake up sleeping threads and modify the sleep list.

---- RATIONALE ----

>> A6: Why did you choose this design?  In what ways is it superior to
>> another design you considered?

We chose this design because it does a fairly low amount of processing in the
timer interrupt handler. As this is called every tick, this is important. Our
initial design used an unordered list of sleeping threads, and each thread
stored the number of ticks to sleep for. In the timer interrupt handler we
would decrement the number of ticks each thread in the sleeping queue was
sleeping for, and then wake up the thread when this count hit zero. This was
really inefficient because we were not just iterating over every sleeping
thread at every tick, but we were also modifying the state of every thread
at every tick.

             PRIORITY SCHEDULING
             ===================

---- DATA STRUCTURES ----

>> B1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

Added to `struct thread':

    struct list lock_list:
        This is an ordered list of the thread's held locks, with 
        highest priority first. They are only added when they have also been 
        donated to.

    int priority:
        This contains the thread's default, non-donated-to, priority.

    struct lock *blocker:
        This is a pointer to the lock that is currently blocking
        it, or NULL should the thread not be currently blocked by 
        a lock.
        
Added to `struct lock':

    bool donated_flag:
        This is a flag to indidicate whether the lock has donated its
        priority to its holder, or not.

    struct list_elem elem:
        This is a list_elem structure such that the lock can be tracked via the 
        pintos list structure - meaning a thread can know which locks it 
        currently holds.

Added to `struct semaphore'
    
    int *priority:
        This is a pointer to a thread's priority value, this is initially the 
        thread that holds it, but is updated to the greatest priority of the 
        thread's currently in it's wait queue.


>> B2: Explain the data structure used to track priority donation.
>> Use ASCII art to diagram a nested donation.  (Alternately, submit a
>> .png file.)

Priority Donation is controlled via each thread containing a list
of pointers to the locks it currently holds, should those locks also have been donated to. The logic behind donation requires that the lock is only donated to and added to this list when the donation priority is greater than the thread's current priority. As such, the thread's current priority is determined in two checks:

If the lock_list is empty, the thread's priority is determined via
thread->priority 
Else, the thread's priority is determined by the head of the lock list's 
priority.

The lock's priority is held in the lock's semaphore. This priority is updated 
to the greatest priority of all the thread's currently waiting on the semaphore.
 As this priority is held in a pointer, an update to the pointer's location to 
a higher priority thread iterates through into the scheduling logic. However, 
the lock is only donated once, on the first instance of the priority donation. 
After this, only its priority pointer updates should a higher priority thread 
attempt to acquire.

Whenever a higher priority thread tries to acquire a lock, the lock pointer is 
`donated' to the lock holder. It is inserted in an ordered way (via descending 
priority) to the thread's lock list.

Should the lock holder, currently be blocked, and the thread's blocking-lock's 
holder be lower priority, then the currently blocking-lock has its priority
pointer updated to point to the thread which has this higher priority. This 
logic recurses, updating the blocking locks in a chain.

--------------------------------------------------------------------------------
--------------------------------- ASCII Diagram: -------------------------------
--------------------------------- Initial Setup: -------------------------------
--------------------------------------------------------------------------------
- Locks: |     A     |     B     |- Threads: |    L     |    M     |    H     |-
---------|pri:NULL   |pri:NULL   |-----------|pri: 1    |pri: 2    |pri: 3    |-
---------|holder:NULL|holder:NULL|-----------|locks:[]  |locks:[]  |locks:[]  |-
---------------------------------------------|blocker:0 |blocker:0 |blocker:0 |
--------------------------------------------------------------------------------
-thread_get_priority(L)=1 - thread_get_priority(M)=2 - thread_get_priority(H)=3-
================================================================================
-- Thread L: lock_acquire(A); ----- Thread M: lock_acquire(b) ------------------
--------------------------------------------------------------------------------
- Locks: |     A     |     B     |- Threads: |    L     |    M     |    H     |-
---------|pri: L->pri|pri: M->pri|-----------|pri: 1    |pri: 2    |pri: 3    |-
---------|holder: L  |holder: M  |-----------|locks:[]  |locks:[]  |locks:[]  |-
---------------------------------------------|blocker:0 |blocker:0 |blocker:0 |-
--------------------------------------------------------------------------------
-thread_get_priority(L)=1 - thread_get_priority(M)=2 - thread_get_priority(H)=3-
================================================================================
-- Thread M: lock_acquire(a) ---------------------------------------------------
--------------------------------------------------------------------------------
- Locks: |     A     |     B     |- Threads: |    L     |M-blocked |    H     |-
---------|pri: M->pri|pri: M->pri|-----------|pri: 1    |pri: 2    |pri: 3    |-
---------|holder: L  |holder: M  |-----------|locks:[A] |locks:[]  |locks:[]  |-
---------------------------------------------|blocker:0 |blocker:&L|blocker:0 |-
--------------------------------------------------------------------------------
-thread_get_priority(L)=2 - thread_get_priority(M)=2 - thread_get_priority(H)=3-
================================================================================
-- Thread H: lock_acquire(b) ---------------------------------------------------
--------------------------------------------------------------------------------
- Locks: |     A     |     B     |- Threads: |    L     |M-blocked |H-blocked |-
---------|pri: H->pri|pri: H->pri|-----------|pri: 1    |pri: 2    |pri: 3    |-
---------|holder: L  |holder: M  |-----------|locks:[A] |locks:[B] |locks:[]  |-
---------------------------------------------|blocker:0 |blocker:&L|blocker:&M|-
--------------------------------------------------------------------------------
-thread_get_priority(L)=3 - thread_get_priority(M)=3 - thread_get_priority(H)=3-
================================================================================
-- Thread L: lock_release(a) -- about to unblock Thread M: lock_acquire(a) -----
--------------------------------------------------------------------------------
- Locks: |     A     |     B     |- Threads: |    L     |M-blocked |H-blocked |-
---------|pri: NULL  |pri: H->pri|-----------|pri: 1    |pri: 2    |pri: 3    |-
---------|holder:NULL|holder: M  |-----------|locks:[]  |locks:[B] |locks:[]  |-
---------------------------------------------|blocker:0 |blocker:&L|blocker:&M|-
--------------------------------------------------------------------------------
-thread_get_priority(L)=3 - thread_get_priority(M)=3 - thread_get_priority(H)=3-
================================================================================
-- Thread M: lock_acquire(a) completes -----------------------------------------
--------------------------------------------------------------------------------
- Locks: |     A     |     B     |- Threads: |    L     |    M     |H-blocked |-
---------|pri: M->pri|pri: H->pri|-----------|pri: 1    |pri: 2    |pri: 3    |-
---------|holder: M  |holder: M  |-----------|locks:[]  |locks:[B] |locks:[]  |-
---------------------------------------------|blocker:0 |blocker:0 |blocker:&M|-
--------------------------------------------------------------------------------
-thread_get_priority(L)=1 - thread_get_priority(M)=3 - thread_get_priority(H)=3-
================================================================================
--And so on and so forth -------------------------------------------------------


---- ALGORITHMS ----

>> B3: How do you ensure that the highest priority thread waiting for
>> a lock, semaphore, or condition variable wakes up first?

The waiters list for a semaphore is ordered by descending priority. Thus the highest priority thread wakes first.

>> B4: Describe the sequence of events when a call to lock_acquire()
>> causes a priority donation.  How is nested donation handled?

Consider the case where there is a data structure as below:

- Locks: |     A     |     B     |- Threads: |    L     |M-blocked |    H     |-
---------|pri: M->pri|pri: M->pri|-----------|pri: 1    |pri: 2    |pri: 3    |-
---------|holder: L  |holder: M  |-----------|locks:[A] |locks:[]  |locks:[]  |-
---------------------------------------------|blocker:0 |blocker:&L|blocker:0 |-
--------------------------------------------------------------------------------
-thread_get_priority(L)=2 - thread_get_priority(M)=2 - thread_get_priority(H)=3-

Should High priority thread (H) call lock_acquire(B), which is held by thread M, the sequence of events is such:
  (1) lock_available(B) returns false
  (2) The priorities of Thread M and thread H are compared
  (3) Since the priority of thread H is greater the priority of thread M
    (4) lock B's priority pointer is set to point to thread H
    (5) Since lock B hasn't been donated to thread M before:
      (6) lock B is donated to thread M (Function call) -->.
        (7) lock B is inserted into thread M's lock_list
        (8) Since thread M is blocked, and the blocker has lower priority than
        |   lock B's priority pointer
        | (9) Thread M's blocker's priority pointer is updated to lock B's 
        |     priority pointer.
        â””---(Recursion Step on M = M->blocker->holder)

At the end of this logical sequence, the data structure would be so:

- Locks: |     A     |     B     |- Threads: |    L     |M-blocked |H-blocked |-
---------|pri: H->pri|pri: H->pri|-----------|pri: 1    |pri: 2    |pri: 3    |-
---------|holder: L  |holder: M  |-----------|locks:[A] |locks:[B] |locks:[]  |-
---------------------------------------------|blocker:0 |blocker:&L|blocker:&M|-
--------------------------------------------------------------------------------
-thread_get_priority(L)=3 - thread_get_priority(M)=3 - thread_get_priority(H)=3-

Nested donation is handled via the recursive step.

>> B5: Describe the sequence of events when lock_release() is called
>> on a lock that a higher-priority thread is waiting for.

Considering the case where data structure is as such:

--------------------------------------------------------------------------------
- Locks: |     A     |     B     |- Threads: |    L     |M-blocked |H-blocked |-
---------|pri: H->pri|pri: H->pri|-----------|pri: 1    |pri: 2    |pri: 3    |-
---------|holder: L  |holder: M  |-----------|locks:[A] |locks:[B] |locks:[]  |-
---------------------------------------------|blocker:0 |blocker:&L|blocker:&M|-
--------------------------------------------------------------------------------
-thread_get_priority(L)=3 - thread_get_priority(M)=3 - thread_get_priority(H)=3-

Should lock_release(A), be called by Thread L:
  (1) Since A's priority has been been donated to L
    (2) thread_restore_priority_lock(A) is called
      (3) list_remove(A->elem) is called to remove A from L's lock_list
  (4) The lock's priority is set to NULL
  (5) The lock's holder is set to NULL
  (6) sema_up(A->semaphore) is called
    (7)  The list of waiters is ensured sorted by descending priority
    (8)  The top priority waiter is popped off the list
    (9)  This waiter is unblocked

---- SYNCHRONIZATION ----

>> B6: Describe a potential race in thread_set_priority() and explain
>> how your implementation avoids it.  Can you use a lock to avoid
>> this race?



---- RATIONALE ----

>> B7: Why did you choose this design?  In what ways is it superior to
>> another design you considered?

Speed, Works..etc

              ADVANCED SCHEDULER
              ==================

---- DATA STRUCTURES ----

>> C1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

/* # of timer ticks until we have to recompute the thread priorities */
static long long mlfqs_recompute_ticks;

Added as a static variable to thread.c:

/* The queue used for the mlfqs scheduler. */
static struct list thread_mlfqs_queue;

/* # of timer ticks until the thread priorities will be recomputed. */
static long long mlfqs_recompute_ticks;

/* The system load average. */
static fixed_point thread_mlfqs_load_avg;

Added to struct thread:

/* The nice value used for the mlfq scheduler. */
int nice;

/* The recent CPU value used by the mlfq scheduler. */
int recent_cpu;

---- ALGORITHMS ----

>> C2: Suppose threads A, B, and C have nice values 0, 1, and 2.  Each
>> has a recent_cpu value of 0.  Fill in the table below showing the
>> scheduling decision and the priority and recent_cpu values for each
>> thread after each given number of timer ticks:

timer  recent_cpu    priority   thread
ticks   A   B   C   A   B   C   to run
-----  --  --  --  --  --  --   ------
 0      0   0   0  63  61  59      A
 4      4   0   0  62  61  59      A  
 8      8   0   0  61  61  59      B
12      8   4   0  61  60  59      A
16      12  4   0  60  60  59      B
20      12  8   0  60  59  59      A     
24      16  8   0  59  59  59      C
28      16  8   4  59  59  58      B 
32      16  12  4  59  58  58      A        
36      20  12  4  58  58  58      C

>> C3: Did any ambiguities in the scheduler specification make values
>> in the table uncertain?  If so, what rule did you use to resolve
>> them?  Does this match the behaviour of your scheduler?

The only possible ambiguity is the coiche of which thread to run when
two or more thread have the same priority. However the specification
says that if there is more than one thread with the same priority, 
the scheduler must go through them following a "round robin" order.
In our implementation, when a thread yields, it is put back into the 
list of ready threads, and we took care of inserting after all the threads 
with the same priority. 

>> C4: How is the way you divided the cost of scheduling between code
>> inside and outside interrupt context likely to affect performance?

---- RATIONALE ----
l
>> C5: Briefly critique your design, pointing out advantages and
>> disadvantages in your design choices.  If you were to have extra
>> time to work on this part of the task, how might you choose to
>> refine or improve your design?

>> C6: The assignment explains arithmetic for fixed-point mathematics in
>> detail, but it leaves it open to you to implement it.  Why did you
>> decide to implement it the way you did?  If you created an
>> abstraction layer for fixed-point mathematics, that is, an abstract
>> data type and/or a set of functions or macros to manipulate
>> fixed-point numbers, why did you do so?  If not, why not?

At the beginning, the fixed-point representation was implemented storing the
actual value of the number in a struct, and using several functions to 
perform the necessary operations. Even if the implementation was correct, 
it was judged not efficient by the members of the group: having to call
a function only to perform a mathematic operation was cause of a huge 
time loss, especially considering that each second, the kernel needs to 
recompute the priority of each thread. Moreover, the memory usage due to
representing the number with a struct could be easily avoided. In the end, 
we decided to define fixed_point as a typedef of an int32_t, and then define
a series of inline functions to actually implement the methods needed to perform
the operations. This strategy was preferred to impementing macros because inline
functions are more pratical when dealing with adding assertions or bits of codes.   


               SURVEY QUESTIONS
               ================

Answering these questions is optional, but it will help us improve the
course in future quarters.  Feel free to tell us anything you
want--these questions are just to spur your thoughts.  You may also
choose to respond anonymously in the course evaluations at the end of
the quarter.

>> In your opinion, was this assignment, or any one of the three problems
>> in it, too easy or too hard?  Did it take too long or too little time?

>> Did you find that working on a particular part of the assignment gave
>> you greater insight into some aspect of OS design?

>> Is there some particular fact or hint we should give students in
>> future quarters to help them solve the problems?  Conversely, did you
>> find any of our guidance to be misleading?

>> Do you have any suggestions for the TAs to more effectively assist
>> students, either for future quarters or the remaining tasks?

>> Any other comments?
